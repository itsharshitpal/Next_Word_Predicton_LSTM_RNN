{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description: Next Word Prediction Using LSTM\n",
    "#### Project Overview:\n",
    "\n",
    "This project aims to develop a deep learning model for predicting the next word in a given sequence of words. The model is built using Long Short-Term Memory (LSTM) networks, which are well-suited for sequence prediction tasks. The project includes the following steps:\n",
    "\n",
    "1- Data Collection: We use the text of Shakespeare's \"Hamlet\" as our dataset. This rich, complex text provides a good challenge for our model.\n",
    "\n",
    "2- Data Preprocessing: The text data is tokenized, converted into sequences, and padded to ensure uniform input lengths. The sequences are then split into training and testing sets.\n",
    "\n",
    "3- Model Building: An LSTM model is constructed with an embedding layer, two LSTM layers, and a dense output layer with a softmax activation function to predict the probability of the next word.\n",
    "\n",
    "4- Model Training: The model is trained using the prepared sequences, with early stopping implemented to prevent overfitting. Early stopping monitors the validation loss and stops training when the loss stops improving.\n",
    "\n",
    "5- Model Evaluation: The model is evaluated using a set of example sentences to test its ability to predict the next word accurately.\n",
    "\n",
    "6- Deployment: A Streamlit web application is developed to allow users to input a sequence of words and get the predicted next word in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Collection\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import  pandas as pd\n",
    "\n",
    "## load the dataset\n",
    "data=gutenberg.raw('shakespeare-hamlet.txt')\n",
    "## save to a file\n",
    "with open('hamlet.txt','w') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##Load the dataset\n",
    "with open('hamlet.txt','r') as file:\n",
    "    text=file.read().lower()\n",
    "\n",
    "## Tokenize the text :It creates indexes for words\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words=len(tokenizer.word_index)+1\n",
    "total_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create input sequences\n",
    "input_sequences=[]\n",
    "for line in text.split('\\n'):\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence=token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pad Sequences\n",
    "max_sequence_len=max([len(x) for x in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create predictors and label\n",
    "import tensorflow as tf\n",
    "x,y=input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.keras.utils.to_categorical(y,num_classes=total_words)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train LSTM RNN\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,GRU\n",
    "\n",
    "## Define the model\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "# #Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRU RNN\n",
    "## Define the model\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "model.add(GRU(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(100))\n",
    "model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "# #Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n",
    "history=model.fit(x_train,y_train,epochs=50,validation_data=(x_test,y_test),verbose=1,callbacks=[early_stopping])\n",
    "\n",
    "# For better accuracy this needs to be trained for longer periods (i.e. we need to increase epochs to say 100 or 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"To be or not to be\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word Prediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "model.save(\"next_word_lstm.h5\")\n",
    "## Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"  Barn. Last night of all,When yond same\"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word PRediction:{next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
